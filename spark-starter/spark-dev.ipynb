{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following getting started docs from: https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "## Dataframe creation\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implicit schema\n",
    "df_i = spark.createDataFrame([\n",
    "    Row(a=1,b=2., c=\"string\", d=date(2022,1,1), e=datetime(2000,1,1,12,0))\n",
    "])\n",
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfeb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explicit schema\n",
    "df_e = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c=\"string\", d=date(2022,1,1), e=datetime(2000,1,1,12,0))\n",
    "], schema=\"a long , b double, c string, d date, e timestamp\")\n",
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7174e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas df to Spark df\n",
    "pandas_df = pd.DataFrame({\n",
    "    \"a\": [1,2,3],\n",
    "    \"b\": [1., 2., 3.]\n",
    "})\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all data from executors/node to the Driver. \n",
    "## Note: You should know what is RDD and what Spark architecture looks like.\n",
    "spark_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'Pandas on Spark' dataframe - Pandas API on spark\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "ps_df = ps.DataFrame({\n",
    "    \"a\": [1,2,3],\n",
    "    \"b\": [\"one\", \"two\", \"three\"]\n",
    "})\n",
    "type(ps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Pandas to 'Pandas on Spark' dataframe\n",
    "pd_df = pd.DataFrame({\n",
    "    \"a\": [1,2,3],\n",
    "    \"b\": [\"one\", \"two\", \"three\"]\n",
    "})\n",
    "sp_df = ps.from_pandas(pd_df)\n",
    "type(sp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce42195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
